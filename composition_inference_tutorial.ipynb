{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Example of how to use the scripts (low-level)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e7bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = 1\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 2\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 3\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 4\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 5\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 6\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 7\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 8\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 9\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 10\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 11\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 12\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 13\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 14\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 15\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 16\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 17\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 18\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 19\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 20\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 21\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 22\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 23\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 24\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 25\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n",
      "Z = 26\n",
      "logE_bin:  17.9_18.0\n",
      "logE_bin:  18.0_18.1\n",
      "logE_bin:  18.1_18.2\n",
      "logE_bin:  18.2_18.3\n",
      "logE_bin:  18.3_18.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from detector_effects import DetectorEffects\n",
    "\n",
    "# Load raw data\n",
    "data_raw = np.load('dataset_test/EPOS_xmax_Ebin2.npy', allow_pickle=True)\n",
    "\n",
    "# Choose energy range log10(E/eV)\n",
    "logE_start = 18\n",
    "logE_end = 18.3\n",
    "\n",
    "# Add detector effects\n",
    "deteff = DetectorEffects(data=data_raw, logE_start=logE_start, logE_end=logE_end)\n",
    "data_simulated = deteff.include(data_unit='EeV')\n",
    "num_primaries = len(data_simulated)\n",
    "# (!) Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcccf18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99995857e-01 6.81360554e+02 2.31608827e+01 1.80270602e+01]\n",
      " [1.00003491e+00 6.55945426e+02 2.31589632e+01 1.80083319e+01]\n",
      " [7.28021997e-01 9.18953368e+02 2.31978513e+01 1.80998231e+01]\n",
      " [9.99945378e-01 7.04294194e+02 2.31571560e+01 1.80789816e+01]\n",
      " [9.99273763e-01 7.22734009e+02 2.31537402e+01 1.80008485e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Shape: #primaries, #events, #features\n",
    "# features: weight, Xmax, dXmax, log_10(E/eV)\n",
    "# Preview first 5 events of the first primary\n",
    "print(data_simulated[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddbe499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Z = 1\n",
      "Primary Z = 2\n",
      "Primary Z = 3\n",
      "Primary Z = 4\n",
      "Primary Z = 5\n",
      "Primary Z = 6\n",
      "Primary Z = 7\n",
      "Primary Z = 8\n",
      "Primary Z = 9\n",
      "Primary Z = 10\n",
      "Primary Z = 11\n",
      "Primary Z = 12\n",
      "Primary Z = 13\n",
      "Primary Z = 14\n",
      "Primary Z = 15\n",
      "Primary Z = 16\n",
      "Primary Z = 17\n",
      "Primary Z = 18\n",
      "Primary Z = 19\n",
      "Primary Z = 20\n",
      "Primary Z = 21\n",
      "Primary Z = 22\n",
      "Primary Z = 23\n",
      "Primary Z = 24\n",
      "Primary Z = 25\n",
      "Primary Z = 26\n"
     ]
    }
   ],
   "source": [
    "import moment_inference as inf\n",
    "\n",
    "# compute non-central moments of Xmax via bootstrap method\n",
    "non_central_moments = inf.bootstrap_nonC_simulated(data_simulated, \n",
    "                                                   num_bootstrap_samples=10000, \n",
    "                                                   num_bootstrap_features=3,  # number of moments\n",
    "                                                   proj_factor=1) \n",
    "\n",
    "# (!) Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c724046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples of central moments for simulated data, given composition w:\n",
    "# Define a composition: 30% proton (Z=1), 10% lithium (Z=3), 60% oxygen (Z=8)\n",
    "\n",
    "w = np.zeros(26)\n",
    "w[0] = 0.3\n",
    "w[2] = 0.1\n",
    "w[7] = 0.6\n",
    "z_samples_sim = inf.central_moments_simulated(w, non_central_moments, num_moments = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a18bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# z_samples_sim is of shape: (num_bootstrap_samples, num_moments):\n",
    "print(z_samples_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e422a21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(!) We use the same events for simulated and measured data only to show how to use the library,\\n    methods of inferring the composition should of course be tested with synthetic measured data which\\n    is composed of events that are not present in simulated data.\\n\\n\\nFor measured data one can use publicly available data from Pierre Auger Open Data: https://opendata.auger.org/\\nIf doing so, parse the data in the following way:\\n\\n    data_measured.shape --> (number of measured events, 4)\\n    features: E, dE, Xmax, dXmax\\n\\nThen compute samples of central moments (no need to compute non-central moments separatelly for measured data)\\n\\nE_min = 1.0 (units EeV)\\nE_max = 2.0 (units_EeV)\\n\\nz_samples_auger = inf.bootstrap_measured(data_measured, E_min, E_max,\\n                                           num_bootstrap_samples=10000,\\n                                           num_bootstrap_features=3,\\n                                           proj_factor=1)\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "As an example, consider `z_samples_sim` with the composition:\n",
    "30% proton (Z=1), 10% lithium (Z=3), 60% oxygen (Z=8)\n",
    "as measured data.\n",
    "\"\"\"\n",
    "z_samples_auger = z_samples_sim.copy() \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "(!) The same events are used for simulated and measured data here\n",
    "    only to demonstrate how to use the library.\n",
    "    In practice, methods for inferring the composition should be tested\n",
    "    with synthetic measured data composed of events that are not present\n",
    "    in the simulated dataset.\n",
    "\n",
    "For measured data, one can use publicly available data from\n",
    "Pierre Auger Open Data: https://opendata.auger.org/\n",
    "\n",
    "If using that data, parse it as follows:\n",
    "\n",
    "    data_measured.shape --> (number of measured events, 4)\n",
    "    Features: E, dE, Xmax, dXmax\n",
    "\n",
    "Then compute samples of central moments.\n",
    "There is no need to compute non-central moments separately for measured data.\n",
    "\"\"\"\n",
    "\n",
    "E_min = 1.0 (units EeV)\n",
    "E_max = 2.0 (units_EeV)\n",
    "\n",
    "z_samples_auger = inf.bootstrap_measured(data_measured, E_min, E_max,\n",
    "                                           num_bootstrap_samples=10000,\n",
    "                                           num_bootstrap_features=3,\n",
    "                                           proj_factor=1)\n",
    "\n",
    "(!) Save results\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa18445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log-likelihood:\n",
    "log_likelihood = inf.define_loglikelihood(num_moments = 3, \n",
    "                                            non_cent_moments_sim = non_central_moments, \n",
    "                                            central_moments_exp  = z_samples_auger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa93660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the prior (Flat Dirichlet). \n",
    "For more information on constructing general priors, see:\n",
    "https://johannesbuchner.github.io/UltraNest/priors.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def dir_prior(quantiles):\n",
    "    gamma_quantiles = -np.log(quantiles)\n",
    "    return gamma_quantiles/gamma_quantiles.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use nested sampling to sample from the posterior, which is proportional to\n",
    "log-likelihood × prior. In this example, we use UltraNest:\n",
    "https://johannesbuchner.github.io/UltraNest/using-ultranest.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#______set parameters\n",
    "live_points = 400\n",
    "dlogz = 0.5 + 0.25*num_primaries\n",
    "results_dir = 'results_dir/'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "sampler = ultranest.ReactiveNestedSampler(param_names, Log_likelihood, transform = dir_prior,\n",
    "    log_dir = results_dir, # folder where to store files\n",
    "    resume = True, # whether to resume from there (otherwise start from scratch)\n",
    "    # warmstart_max_tau = 0.5,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "sampler.stepsampler = ultranest.stepsampler.SliceSampler(\n",
    "                       nsteps = 100,\n",
    "                       generate_direction = ultranest.stepsampler.generate_mixture_random_direction,\n",
    "                       )\n",
    "\n",
    "\n",
    "result = sampler.run(\n",
    "                   min_num_live_points = live_points,\n",
    "                   dlogz = dlogz, # desired accuracy on logz\n",
    "                   min_ess = live_points, # number of effective samples\n",
    "                   max_num_improvement_loops = 3, # how many times to go back and improve\n",
    "                  )\n",
    "\n",
    "\n",
    "compositions = result['weighted_samples']['points']\n",
    "weights      = result['weighted_samples']['weights']\n",
    "logL         = result['weighted_samples']['logl']\n",
    "# (!) Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order samples from lowest to highest log-likelihood:\n",
    "composition_sorted, confidence_levels, logl_values_sorted = sort_data(compositions, weights, logL)\n",
    "\n",
    "# The most probable composition is:\n",
    "composition_best_estimate = composition_sorted[-1]  # equivalent to: composition[np.argsort(logL)[-1]]\n",
    "print(composition_best_estimate)\n",
    "\n",
    "print('Original composition: ', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Plot the confidence level (posterior volume from highest to lowest likelihood)\n",
    "as a function of the log-likelihood value.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(confidence_levels, logl_values_sorted, color = 'black')\n",
    "plt.xlabel('Log-Likelihood value')\n",
    "plt.ylabel('Confidence level')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To obtain the truly most probable composition, find the composition w\n",
    "that maximizes log-likelihood × prior.\n",
    "\n",
    "To obtain the 95% confidence interval for the proton fraction:\n",
    "1. Find the compositions with the lowest and highest proton fractions\n",
    "   among those with log-likelihood ≥ logL at confidence level = 0.95.\n",
    "2. These bounds define the 95% confidence interval for the proton fraction.\n",
    "\n",
    "A similar approach can be used to compute confidence intervals\n",
    "for more complicated quantities derived from the posterior.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next (to be added):\n",
    "- Classification of events into primary particles based on Xmax\n",
    "- Machine learning methods for event classification and composition analysis\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
