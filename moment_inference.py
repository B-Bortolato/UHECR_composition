#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 19 22:59:29 2025

@author: blaz





"""


import numpy as np


def bootstrap_nonC_simulated(data_simulated, 
                             num_bootstrap_samples=10000, 
                             num_bootstrap_features=3, 
                             proj_factor=1):
    """
    Estimate non-central moments of the X_max distribution for each primary species
    using a weighted bootstrap procedure applied to detector-level simulated data.

    Parameters
    ----------
    data_simulated : list of np.ndarray
        A list containing one array per primary mass/charge group.  
        Each array has shape (N_events, 4) and columns:
            0: event weight,
            1: X_max value,
            2: X_max detector uncertainty (σ_Xmax),
            3: log10(E/eV).
        These are detector-level (not truth-level) simulations and include all detector effects.

    num_bootstrap_samples : int, optional
        Number of bootstrap resamples to generate for each primary.  
        Each resample produces a full set of non-central moments.

    num_bootstrap_features : int, optional
        Number of non-central moments to compute. The current implementation
        expects 6 (moments of order 1–6) and will return an array with that size.

    proj_factor : float, optional
        Scaling factor applied to the number of resampled events.
        The bootstrap draws `int(N_events * proj_factor)` samples with replacement.
        Default value: 1. If value > 1 is equivalent to computing non-central moments
        with n-times more simulated data; where proj_factor=n (use for projections).
        
    Returns
    -------
    non_cent_moments_sim : np.ndarray
        Array of shape (num_primaries, num_bootstrap_samples, num_bootstrap_features).  
        Entry `[z, k, :]` contains the first six weighted non-central moments of X_max
        for primary index `z` in bootstrap iteration `k`.

    Method
    ------
    For each primary species:
      * Extract weights, X_max, X_max uncertainties, and log10(E).
      * For each bootstrap iteration:
          - Draw event indices with replacement.
          - Add Gaussian noise `ε ~ N(0,σ)` to the selected X_max values using their
            event-wise uncertainties (this implicitly assumes a linear, Gaussian detector error model).
          - Recompute weighted non-central moments:
                m_n = Σ(w_i * X_i^n) / Σ(w_i)   for n = 1…6.
      * Moments across bootstrap iterations provide an empirical distribution of
        detector-smeared non-central moments.


    """

    num_primaries = len(data_simulated)
    non_cent_moments_sim = np.zeros((num_primaries, num_bootstrap_samples , num_bootstrap_features))
    for z in range(num_primaries):
        print(f'Primary Z = {z+1}')
        
        norm__, xmax, dxmax, logEz = data_simulated[z].T
    

        for k in range(num_bootstrap_samples):
            epsilon  = np.random.normal(loc = 0.0, scale = 1.0, size = int(len(xmax)*proj_factor))
            RND = np.random.choice(len(xmax), replace = True,   size = int(len(xmax)*proj_factor))
            
            xmax_sample = xmax[RND] + dxmax[RND] * epsilon 
            norm = norm__[RND]  
            normsum = norm.sum()
    
            z1 = np.sum( xmax_sample    * norm) / normsum 
            z2 = np.sum( xmax_sample**2 * norm) / normsum 
            z3 = np.sum( xmax_sample**3 * norm) / normsum 
            z4 = np.sum( xmax_sample**4 * norm) / normsum 
            z5 = np.sum( xmax_sample**5 * norm) / normsum
            z6 = np.sum( xmax_sample**6 * norm) / normsum
            non_cent_moments_sim[z, k] = [z1, z2, z3, z4, z5, z6]
    return non_cent_moments_sim
    









def central_moments_simulated(w, non_cent_moments_sim, num_moments):
    """
    Compute the bootstrap-distributed CENTRAL moments of the X_max mixture
    implied by a given primary composition vector.
    
    Parameters
    ----------
    w : array-like, shape (num_primaries,)
        Composition weights for each primary species.  
        Should sum to 1, but the function does not enforce normalization.
        Example:
            100% proton → w = [1, 0, 0, ...].
    
    non_cent_moments_sim : np.ndarray
        Array of bootstrap-resampled **non-central moments** generated by
        `bootstrap_method`.  
        Shape: (num_primaries, num_bootstrap_samples, num_moments_available).  
        Entry [z, k, n] = n-th non-central moment of X_max for primary z in
        bootstrap iteration k.
    
    num_moments : int
        Number of CENTRAL moments to compute.  
        Supported values: 1–6.  
        The function returns:
            num_moments = 1 → mean  
            num_moments = 2 → (mean, variance)  
            num_moments = 3 → (mean, variance, third central moment)  
            ... up to the sixth central moment.
    
    Returns
    -------
    Z_samples : np.ndarray
        Array of shape (num_bootstrap_samples, num_moments).  
        For each bootstrap sample k, the row contains:
            (z1, z2, ..., z_num_moments)
        where z_n are **central** moments of the X_max distribution
        produced by the mixture composition w.
    
    Method
    ------
    For each central moment order n:
    
      1. Compute the composition-weighted non-central moment:
            m_n(k) = Σ_z [ w_z * M_n(z, k) ]
         where M_n(z, k) is the n-th non-central moment for primary z
         in bootstrap iteration k.
    
      2. Convert non-central moments m_n to central moments z_n using
         the standard polynomial relations:
            z2 = m2 - m1²  
            z3 = m3 - 3 m2 m1 + 2 m1³  
            ...  
         (implemented explicitly up to n = 3).
    
      3. Stack the resulting values into a sample-by-moment matrix.
      """
      
      
    Gn = non_cent_moments_sim
    
    if num_moments == 1:
        Z_samples = np.sum( Gn[:,:,0] * w.reshape(-1,1), axis = 0)
        
        
    elif num_moments == 2:
        m1   = np.sum( Gn[:,:,0] * w.reshape(-1,1), axis = 0)
        m2   = np.sum( Gn[:,:,1] * w.reshape(-1,1), axis = 0)
        
        z1 = m1
        z2 = m2 - m1**2
        Z_samples = np.vstack([z1, z2]).T
        
    elif num_moments == 3:
    
        m1   = np.sum( Gn[:,:,0] * w.reshape(-1,1), axis = 0)
        m2   = np.sum( Gn[:,:,1] * w.reshape(-1,1), axis = 0)
        m3   = np.sum( Gn[:,:,2] * w.reshape(-1,1), axis = 0)
        
        z1 = m1
        z2 = m2 - m1**2
        z3 = m3 - 3*m2*m1 + 2*m1**3
        Z_samples = np.vstack([z1, z2, z3]).T
        
    
        
    elif num_moments == 4:
        m1   = np.sum( Gn[:,:,0] * w.reshape(-1,1), axis = 0)
        m2   = np.sum( Gn[:,:,1] * w.reshape(-1,1), axis = 0)
        m3   = np.sum( Gn[:,:,2] * w.reshape(-1,1), axis = 0)
        m4   = np.sum( Gn[:,:,3] * w.reshape(-1,1), axis = 0)
        
        z1 = m1
        z2 = m2 - m1**2
        z3 = m3 - 3*m2*m1 + 2*m1**3
        z4 = m4 - 4*m3*m1 + 6*m2*m1**2 - 3*m1**4
        Z_samples = np.vstack([z1, z2, z3, z4]).T
        
        
        
    elif num_moments == 5:
        m1   = np.sum( Gn[:,:,0] * w.reshape(-1,1), axis = 0)
        m2   = np.sum( Gn[:,:,1] * w.reshape(-1,1), axis = 0)
        m3   = np.sum( Gn[:,:,2] * w.reshape(-1,1), axis = 0)
        m4   = np.sum( Gn[:,:,3] * w.reshape(-1,1), axis = 0)
        m5   = np.sum( Gn[:,:,4] * w.reshape(-1,1), axis = 0)

        z1 = m1
        z2 = m2 - m1**2
        z3 = m3 - 3*m2*m1 + 2*m1**3
        z4 = m4 - 4*m3*m1 + 6*m2*m1**2 - 3*m1**4
        z5 = m5 - 5*m4*m1 + 10*m3*m1**2 - 10*m2*m1**3 + 4*m1**5
        
        Z_samples = np.vstack([z1, z2, z3, z4, z5]).T
                
        

    elif num_moments == 6:
        m1   = np.sum( Gn[:,:,0] * w.reshape(-1,1), axis = 0)
        m2   = np.sum( Gn[:,:,1] * w.reshape(-1,1), axis = 0)
        m3   = np.sum( Gn[:,:,2] * w.reshape(-1,1), axis = 0)
        m4   = np.sum( Gn[:,:,3] * w.reshape(-1,1), axis = 0)
        m5   = np.sum( Gn[:,:,4] * w.reshape(-1,1), axis = 0)
        m6   = np.sum( Gn[:,:,5] * w.reshape(-1,1), axis = 0)

        z1 = m1
        z2 = m2 - m1**2
        z3 = m3 - 3*m2*m1 + 2*m1**3
        z4 = m4 - 4*m3*m1 + 6*m2*m1**2 - 3*m1**4
        z5 = m5 - 5*m4*m1 + 10*m3*m1**2 - 10*m2*m1**3 + 4*m1**5
        z6 = m6 - 6*m5*m1 + 15*m4*m1**2 - 20*m3*m1**3 + 15*m2*m1**4 - 5*m1**6 
        
        Z_samples = np.vstack([z1, z2, z3, z4, z5, z6]).T
                
        
        
    return Z_samples




def z_mean_cov_simulated(w, non_cent_moments_sim, num_moments):
    """
    Compute the mean vector and covariance matrix of the CENTRAL moments
    of X_max implied by composition w.

    Parameters
    ----------
    w : array-like, shape (num_primaries,)
        Composition weights for each primary component.
    non_cent_moments_sim : np.ndarray
        Bootstrap samples of non-central moments; see `bootstrap_method`.
    num_moments : int
        Number of central moments to compute (1–6).

    Returns
    -------
    z_mean : np.ndarray, shape (num_moments,)
        Sample mean of the central-moment vector across bootstrap samples.
    z_cov : np.ndarray, shape (num_moments, num_moments)
        Sample covariance matrix of central moments.

    Method
    ------
    * Calls `central_moments_simulated(...)` to construct the
      bootstrap distribution of central moments.
    * Computes mean and covariance using standard numpy estimators.
    
    """

    z_samples = central_moments_simulated(w, non_cent_moments_sim, num_moments)
    z_mean = np.mean(z_samples, axis = 0)
    z_cov  = np.cov(z_samples.T)
    return z_mean, z_cov



def central_moments_simulated_1D(w, non_cent_moments_sim):
    """
    Compute the composition-weighted bootstrap distribution of the
    **mean X_max** (first non-central moment) for a given composition vector.

    Parameters
    ----------
    w : array-like, shape (num_primaries,)
        Composition weights for each primary. Should ideally sum to 1.
    Gn : np.ndarray
        Bootstrap-resampled non-central moments for each primary.
        Shape: (num_primaries, num_bootstrap_samples, num_moments_available).
        Only the first non-central moment (index 0) is used.

    Returns
    -------
    Z_samples : np.ndarray, shape (num_bootstrap_samples,)
        The bootstrap-distributed composition-weighted mean X_max values.
        Entry k equals Σ_z w_z * m1(z, k).
    """

    Z_samples  = np.sum( non_cent_moments_sim[:,:,0] * w.reshape(-1,1), axis = 0)    
    return Z_samples   




def z_mean_std_simulated_1D(w, non_cent_moments_sim):
    """
    Compute the mean and standard deviation of the composition-weighted
    bootstrap distribution of the mean X_max.
    
    Parameters
    ----------
    w : array-like, shape (num_primaries,)
        Composition vector.
    Gn : np.ndarray
        Non-central bootstrap samples; only the first moment is used.
    
    Returns
    -------
    z_mean : float
        Average composition-weighted mean X_max across bootstrap samples.
    z_std : float
        Standard deviation across bootstrap samples.
    """
    z_samples = central_moments_simulated_1D(w, non_cent_moments_sim)
    z_mean = np.mean(z_samples)
    z_std  = np.std(z_samples)
    return z_mean, z_std




def clipE(data, E_min, E_max, axis = 0):
    """
    Select (filter) rows of a data array whose energy value lies within 
    the interval [E_min, E_max].
    
    Parameters
    ----------
    data : np.ndarray, shape (N, M)
        Input dataset. Each row corresponds to an event; one of the columns 
        contains the log10(E/eV) values used for filtering.
    
    E_min, E_max : float
        Lower and upper bounds of the allowed energy interval.
    
    axis : int, optional
        Column index of the energy variable in `data`. Defaults to 0.
    
    Returns
    -------
    filtered_data : np.ndarray
        Subset of `data` containing only events satisfying:
            E_min ≤ data[:, axis] ≤ E_max.
    """
    E = data[:,axis]
    con = (E >= E_min) & (E <= E_max)
    return data[con]




def bootstrap_measured(data_measured, E_min, E_max,
                   num_bootstrap_samples=10000,
                   num_bootstrap_features=3,
                   proj_factor=1):
    """
    Perform a bootstrap estimation of CENTRAL moments of X_max from 
    measured (detector-level) data within a given energy range.

    Parameters
    ----------
    data_measured : np.ndarray, shape (N_events, 4)
        Experimental dataset with columns:
            0: log10(E/eV),
            1: log10 energy uncertainty,
            2: measured X_max,
            3: X_max measurement uncertainty (σ_Xmax).
        These are fully detector-level measurements.

    E_min, E_max : float
        Minimum and maximum log10(E/eV) defining the energy selection window.
        (Note: the current implementation does not apply the cut explicitly.)

    num_bootstrap_samples : int, optional
        Number of bootstrap resamples. Defaults to 10,000.

    num_bootstrap_features : int, optional
        Number of central moments to compute (fixed to 3 in current code).

    proj_factor : float, optional
        Intended scaling factor for the number of bootstrap draws (not used
        in the present implementation). For proj_factor > 1, bootstrapping 
        would emulate having more data.

    Returns
    -------
    central_moments_exp : np.ndarray, shape (num_bootstrap_samples, 6)
        For each bootstrap sample k, the row contains:
            (mean, variance, 3rd, 4th, 5th, 6th central moments)
        computed from detector-smeared X_max resamples.

    Method
    ------
    For each bootstrap iteration:
      * Draw event indices with replacement.
      * Perturb X_max according to its event-wise detector uncertainty:
            X'_i = X_i + σ_i * ε_i,   ε_i ~ N(0, 1)
        which imposes a Gaussian detector response model.
      * Compute the first six central moments:
            z1 = mean(X')
            z2 = E[(X'-z1)^2], …, z6 = E[(X'-z1)^6].
      * Store the resulting moment vector.

    """

    
    E, dE, xmax, dxmax = data_measured.T

    #____________________________Bootstrapping EXPERIMENTAL DATA
    proj_factor_exp = 1 # for proj_factor_exp=2 projects boottrapping results as if there is 2x the amount of data available
    num_bootstrap_samples = 10000
    num_bootstrap_features = 6
    

    
    central_moments_exp = np.zeros((num_bootstrap_samples, num_bootstrap_features))
    for k in range(num_bootstrap_samples):
        epsilon  = np.random.normal(loc = 0.0, scale = 1.0,    size = int(len(xmax)*proj_factor_exp))
        RND      = np.random.choice(len(xmax), replace = True, size = int(len(xmax)*proj_factor_exp))
        xmax_sample = xmax[RND] + dxmax[RND] * epsilon
    
        z1 = np.mean( xmax_sample )
        z2 = np.mean( (xmax_sample - z1)**2 )
        z3 = np.mean( (xmax_sample - z1)**3 )
        z4 = np.mean( (xmax_sample - z1)**4 )    
        z5 = np.mean( (xmax_sample - z1)**5 )    
        z6 = np.mean( (xmax_sample - z1)**6 )    
        central_moments_exp[k] = [z1, z2, z3, z4, z5, z6]
    return central_moments_exp



def z_mean_std_measured_1D(central_moments_exp):
    """
    Compute the mean and standard deviation of the measured mean X_max
    from bootstrap-resampled central moments.
    
    Parameters
    ----------
    central_moments_exp : np.ndarray, shape (num_bootstrap_samples, 6)
        Bootstrap samples of central moments derived from measured data.
    
    Returns
    -------
    z_mean_exp : float
        Mean of the bootstrap-distributed mean X_max.
    z_std_exp : float
        Standard deviation of the bootstrap-distributed mean X_max.
    """
    z_mean_exp = np.mean(central_moments_exp, axis = 0)
    z_cov_exp  = np.cov( central_moments_exp.T)
    return z_mean_exp[0], z_cov_exp[0,0]**0.5



def z_mean_cov_measured(central_moments_exp, num_moments):
    """
    Compute the mean vector, covariance matrix, and inverse covariance matrix
    of the first `num_moments` central moments derived from measured data.
    
    Parameters
    ----------
    central_moments_exp : np.ndarray, shape (num_bootstrap_samples, 6)
        Bootstrap samples of central moments.
    num_moments : int
        Number of central moments to return (1–6).
    
    Returns
    -------
    z_mean_measured : np.ndarray, shape (num_moments,)
        Mean of the selected central moments.
    z_cov_measured : np.ndarray, shape (num_moments, num_moments)
        Covariance matrix of the selected central moments.
    inv_cov_measured : np.ndarray, shape (num_moments, num_moments)
        Inverse of the covariance matrix.
    """
    z_mean_exp = np.mean(central_moments_exp, axis = 0)
    z_cov_exp  = np.cov( central_moments_exp.T)
    
    z_mean_measured = z_mean_exp[:num_moments]
    z_cov_measured  = z_cov_exp[:num_moments, :num_moments]
    # inv_cov_measured     = np.linalg.inv(z_cov_measured)
    return z_mean_measured, z_cov_measured#, inv_cov_measured





def log_likelihood(w, num_moments, non_cent_moments_sim, central_moments_exp):
    """
    Construct a log-likelihood function for comparing simulated and measured
    X_max moment distributions, based on a specified number of central moments.

    Parameters
    ----------
    w : array-like
        Composition weights for each primary type.
    num_moments : int
        Number of central moments to use (1–6).
        If 1, a univariate Gaussian likelihood is used.
        If ≥2, a multivariate Gaussian likelihood is formed.
    non_cent_moments_sim : np.ndarray
        Bootstrap-resampled non-central moments for simulated data.
    central_moments_exp : np.ndarray
        Bootstrap-resampled central moments for measured data.

    Returns
    -------
    log_likelihood : callable
        A function `log_likelihood(w)` returning the log-likelihood
        evaluated at composition vector w, using either:
            * the first central moment only (num_moments = 1), or
            * the first `num_moments` central moments (num_moments ≥ 2).
    """
    
    if num_moments == 1:
        z_mean_exp = np.mean(central_moments_exp, axis = 0)
        z_cov_exp  = np.cov( central_moments_exp.T)
        z_aug_mean = z_mean_exp[0]
        z_aug_std = z_cov_exp[0,0]**0.5
        
        
        def log_likelihood_1moment(w):
            """
            Compute the log-likelihood for a single-moment (mean X_max) comparison
            between simulated and measured distributions for a given composition w.
        
            Parameters
            ----------
            w : array-like
                Composition weights.
        
            Returns
            -------
            ll : float
                Log-likelihood value based on the first central moment only.
            """
            z_sim_mean, z_sim_std = z_mean_std_simulated_1D(w, non_cent_moments_sim)
            exp_1st_part = -1/2*(  (z_sim_mean/z_sim_std)**2 + (z_aug_mean/z_aug_std)**2  )
            exp_2nd_part =  1/2 * (z_sim_std * z_aug_std)**2 / (z_sim_std**2 + z_aug_std**2) * (z_sim_mean/z_sim_std**2 + z_aug_mean/z_aug_std**2)**2
            return -1/2 * np.log(2*np.pi * (z_sim_std**2 + z_aug_std**2)) + exp_1st_part + exp_2nd_part
        return log_likelihood_1moment
    
    elif num_moments in [2,3,4,5,6]:
        z_mean_exp = np.mean(central_moments_exp, axis = 0)
        z_cov_exp  = np.cov( central_moments_exp.T)
        
        z_mean_measured = z_mean_exp[:num_moments]
        z_cov_measured  = z_cov_exp[:num_moments, :num_moments]
        inv_cov_measured     = np.linalg.inv(z_cov_measured)
        z_aug_mean = z_mean_measured
        z_aug_cov = z_cov_measured
        inv_cov_aug = inv_cov_measured

        def log_likelihood_xmoments(w):
            """
            Compute the multivariate Gaussian log-likelihood comparing simulated
            and measured central-moment vectors for a given composition w.
        
            Parameters
            ----------
            w : array-like
                Composition weights.
        
            Returns
            -------
            ll : float
                Log-likelihood value using the first `num_moments`
                central moments of X_max.
            """
            z_sim_mean, z_sim_cov = z_mean_cov_simulated(w, non_cent_moments_sim)
            inv_cov_sim = np.linalg.inv(z_sim_cov)
            inv_inv_cov = np.linalg.inv(inv_cov_sim + inv_cov_aug)
            
            
            z_aux = inv_cov_sim.dot(z_sim_mean) +  inv_cov_aug.dot(z_aug_mean)
            
            
            norm = - 1/2*num_moments * np.log(2 *  np.pi)  - 1/2 * np.log(np.linalg.det( z_sim_cov + z_aug_cov))
            exp_1st_part = -1/2 * np.sum(inv_cov_sim.dot(z_sim_mean) * z_sim_mean)
            exp_2nd_part = -1/2 * np.sum(inv_cov_aug.dot(z_aug_mean) * z_aug_mean)
            exp_3rd_part =  1/2 * np.sum(inv_inv_cov.dot(z_aux) * z_aux)
            return norm + exp_1st_part + exp_2nd_part + exp_3rd_part
        return log_likelihood_xmoments



def dir_prior(quantiles):
    """
    Convert a vector of quantiles into a Dirichlet prior by mapping them
    through an exponential transformation and normalizing.
    
    Parameters
    ----------
    quantiles : array-like
        Input quantile values.
    
    Returns
    -------
    prior : np.ndarray
        Normalized Dirichlet prior weights.
    """
    gamma_quantiles = -np.log(quantiles)
    return gamma_quantiles/gamma_quantiles.sum()









        


def sort_data(compositions, weights, logl_values):
    """
    Sort composition samples by log-likelihood and compute cumulative 
    confidence levels based on posterior weights.
    
    Parameters
    ----------
    compositions : np.ndarray, shape (N, ...)
        Array of composition samples corresponding to each log-likelihood value.
    weights : np.ndarray, shape (N,)
        Posterior weights associated with each sample. Must sum to 1.
    logl_values : np.ndarray, shape (N,)
        Log-likelihood values for each composition sample.
    
    Returns
    -------
    composition_sorted : np.ndarray
        Composition samples sorted by descending log-likelihood.
    confidence_levels : np.ndarray
        Cumulative confidence levels for each sorted sample. Highest likelihood
        sample has confidence level ~0; lower likelihood samples have increasing
        confidence.
    logl_values_sorted : np.ndarray
        Log-likelihood values sorted in descending order.
    """
    indices_sorted     = np.argsort(logl_values)
    confidence_levels  = 1 - weights[indices_sorted].cumsum()
    composition_sorted = compositions[indices_sorted]
    logl_values_sorted = logl_values[indices_sorted]
    return composition_sorted, confidence_levels, logl_values_sorted



